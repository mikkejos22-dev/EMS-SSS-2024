from __future__ import print_function
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten


# Directory paths for datasets
main_directory = '30by30 images'  # Directory containing main circuit component images
transfer_directory = 'Transfer Dataset'  # Directory containing transfer dataset images

# Pull and split datasets for main components
component_train = tf.keras.preprocessing.image_dataset_from_directory(
    main_directory,
    validation_split=0.2,
    subset='t1991raining',
    seed=123,
    batch_size=16,
    color_mode='grayscale',
    image_size=(30, 30)
)
component_test = tf.keras.preprocessing.image_dataset_from_directory(
    main_directory,
    validation_split=0.2,
    subset='validation',
    seed=123,
    batch_size=16,
    color_mode='grayscale',
    image_size=(30, 30)
)

# Pull and split datasets for transfer learning
transfer_train = tf.keras.preprocessing.image_dataset_from_directory(
    transfer_directory,
    validation_split=0.2,
    subset='training',
    seed=123,
    batch_size=16,
    color_mode='grayscale',
    image_size=(30, 30)
)
transfer_test = tf.keras.preprocessing.image_dataset_from_directory(
    transfer_directory,
    validation_split=0.2,
    subset='validation',
    seed=123,
    batch_size=16,
    color_mode='grayscale',
    image_size=(30, 30)
)

# Tune training for optimization
AUTOTUNE = tf.data.AUTOTUNE
component_train = component_train.cache().prefetch(buffer_size=AUTOTUNE)
component_test = component_test.cache().prefetch(buffer_size=AUTOTUNE)
transfer_train = transfer_train.cache().prefetch(buffer_size=AUTOTUNE)
transfer_test = transfer_test.cache().prefetch(buffer_size=AUTOTUNE)

# Number of output classes
num_classes = 3

# First model architecture
feature_layers_1 = [
    Conv2D(32, 3, activation='relu', input_shape=(30, 30, 1)),
    MaxPooling2D(),
    Conv2D(32, 3, activation='relu'),
    MaxPooling2D(),
    Conv2D(32, 3, activation='relu'),
    MaxPooling2D(),
    Flatten()
]

classification_layers_1 = [
    Dense(128),
    Activation('relu'),
    Dropout(0.25),
    Dense(num_classes),
    Activation('softmax')
]

# Define and compile first model
model_1 = Sequential(feature_layers_1 + classification_layers_1)
model_1.summary()
model_1.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Train first model on main components
model_1.fit(
    component_train,
    validation_data=component_test,
    epochs=3
)

# Freeze feature layers of the first model for transfer learning
for layer in feature_layers_1:
    layer.trainable = False

# Reconfigure and compile the first model for transfer learning
model_1 = Sequential(feature_layers_1 + classification_layers_1)
model_1.summary()
model_1.compile(
    optimizer='adam',
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=['accuracy']
)

# Fine-tune first model on transfer dataset
model_1.fit(
    transfer_train,
    validation_data=transfer_test,
    epochs=3
)

# Second model architecture
feature_layers_2 = [
    Conv2D(32, 3, activation='relu', input_shape=(30, 30, 1)),
    Conv2D(32, 3, activation='relu'),
    MaxPooling2D(),
    Conv2D(32, 3, activation='relu'),
    Conv2D(32, 3, activation='relu'),
    MaxPooling2D(),
    Flatten(),
]

classification_layers_2 = [
    Dense(128, activation='relu'),
    Dense(128, activation='relu'),
    Dropout(0.25),
    Dense(num_classes),
    Activation('softmax')
]

# Define and compile second model
model_2 = Sequential(feature_layers_2 + classification_layers_2)
model_2.summary()
model_2.compile(
    optimizer='adam',
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=['accuracy']
)

# Train second model on main components
model_2.fit(
    component_train,
    validation_data=component_test,
    epochs=3
)

# Freeze feature layers of the second model for transfer learning
for layer in feature_layers_2:
    layer.trainable = False

# Reconfigure and compile the second model for transfer learning
model_2 = Sequential(feature_layers_2 + classification_layers_2)
model_2.summary()
model_2.compile(
    optimizer='adam',
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=['accuracy']
)

# Fine-tune second model on transfer dataset
model_2.fit(
    transfer_train,
    validation_data=transfer_test,
    epochs=3
)

# Third model architecture
feature_layers_3 = [
    Conv2D(32, 3, activation='relu', input_shape=(30, 30, 1)),
    Conv2D(32, 3, activation='relu'),
    Conv2D(32, 3, activation='relu'),
    MaxPooling2D(),
    Conv2D(32, 3, activation='relu'),
    Conv2D(32, 3, activation='relu'),
    Conv2D(32, 3, activation='relu'),
    MaxPooling2D(),
    Flatten()
]

classification_layers_3 = [
    Dense(512, activation='relu'),
    Dense(256, activation='relu'),
    Dense(128, activation='relu'),
    Dropout(0.25),
    Dense(num_classes),
    Activation('softmax')
]

# Define and compile third model
model_3 = Sequential(feature_layers_3 + classification_layers_3)
model_3.summary()
model_3.compile(
    optimizer='adam',
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=['accuracy']
)

# Train third model on main components
model_3.fit(
    component_train,
    validation_data=component_test,
    epochs=3
)

# Freeze feature layers of the third model for transfer learning
for layer in feature_layers_3:
    layer.trainable = False

# Reconfigure and compile the third model for transfer learning
model_3 = Sequential(feature_layers_3 + classification_layers_3)
model_3.summary()
model_3.compile(
    optimizer='adam',
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=['accuracy']
)

# Fine-tune third model on transfer dataset
model_3.fit(
    transfer_train,
    validation_data=transfer_test,
    epochs=3
)

