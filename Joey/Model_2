from __future__ import print_function
import tensorflow as tf
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification
from sklearn.utils import resample
from sklearn.metrics import accuracy_score
from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint
from keras.optimizers import SGD, RMSprop
from keras.src.callbacks import history
from keras.src.layers import AveragePooling2D
from numpy import argmin
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, Input
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.regularizers import l2
import matplotlib.pyplot as plt
import matplotlib.patches as patches

main_directory = '30by30 images'  # Add personal directory here
transfer_directory = 'Transfer Dataset'  # Add personal directory here

# Load data
component_train = tf.keras.preprocessing.image_dataset_from_directory(
    main_directory,
    validation_split=0.2,
    subset='training',
    seed=123,
    batch_size=16,
    color_mode='grayscale',
    image_size=(30, 30)
)

component_test = tf.keras.preprocessing.image_dataset_from_directory(
    main_directory,
    validation_split=0.3,
    subset='validation',
    seed=123,
    batch_size=16,
    color_mode='grayscale',
    image_size=(30, 30)
)
transfer_train = tf.keras.preprocessing.image_dataset_from_directory(transfer_directory,
                                                                      validation_split = 0.2,
                                                                      subset = 'training',
                                                                      seed = 123,
                                                                      batch_size = 16,
                                                                      color_mode = 'grayscale',
                                                                      image_size = (30,30)
                                                                     )
transfer_test = tf.keras.preprocessing.image_dataset_from_directory(transfer_directory,
                                                                      validation_split = 0.2,
                                                                      subset = 'validation',
                                                                      seed = 123,
                                                                      batch_size = 16,
                                                                      color_mode = 'grayscale',
                                                                      image_size = (30,30)
                                                                     )

# Data Augmentation setup
datagen = ImageDataGenerator(
    rotation_range=35,
    width_shift_range=0.3,
    height_shift_range=0.3,
    shear_range=0.3,
    zoom_range=0.3,
    horizontal_flip=True,
    fill_mode='nearest',
    validation_split=0.3  # Reserve 30% of data for validation
)

# Modify dataset loading to include data augmentation
component_train_augmented = datagen.flow_from_directory(
    main_directory,
    target_size=(30, 30),
    color_mode='grayscale',
    batch_size=16,
    class_mode='sparse',
    subset='training',
    seed=123
)

component_test_augmented = datagen.flow_from_directory(
    main_directory,
    target_size=(30, 30),
    color_mode='grayscale',
    batch_size=16,
    class_mode='sparse',
    subset='validation',
    seed=123
)

transfer_train_augmented = datagen.flow_from_directory(
    transfer_directory,
    target_size=(30, 30),
    color_mode='grayscale',
    batch_size=16,
    class_mode='sparse',
    subset='training',
    seed=123
)

transfer_test_augmented = datagen.flow_from_directory(
    transfer_directory,
    target_size=(30, 30),
    color_mode='grayscale',
    batch_size=16,
    class_mode='sparse',
    subset='validation',
    seed=123
)

# Early stopping callback
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=4,  # Number of epochs with no improvement after which training will be stopped
    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity
)

model_checkpoint = ModelCheckpoint(
    'best_model.keras',
    monitor='val_loss',
    save_best_only=True
)

# Cache and prefetch setup
AUTOTUNE = tf.data.AUTOTUNE
component_train_augmented = component_train.cache().prefetch(buffer_size=AUTOTUNE)
component_test_augmented = component_test.cache().prefetch(buffer_size=AUTOTUNE)
transfer_train_augmented = component_train.cache().prefetch(buffer_size=AUTOTUNE)
transfer_test_augmented = component_test.cache().prefetch(buffer_size=AUTOTUNE)

# Reduce learning rate on plateau: helps with overfitting
reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.15,  # Factor by which the learning rate will be reduced
    patience=3,  # Number of epochs with no improvement after which learning rate will be reduced
    min_lr=0.000001  # Lower bound on the learning rate
)
l2_regularizer = l2(0.001)  # You can adjust the regularization factor

num_classes = 3
# Pretraining: Self-Supervised Rotation Prediction
def create_rotated_dataset(dataset):
    rotated_images = []
    rotated_labels = []
    rotations = [0, 90, 180, 270]
    for images, labels in dataset:
        for image in images:
            for i, angle in enumerate(rotations):
                rotated_images.append(tf.image.rot90(image, k=i))
                rotated_labels.append(i)
    return tf.data.Dataset.from_tensor_slices((rotated_images, rotated_labels))

rotated_train = create_rotated_dataset(transfer_train_augmented)
rotated_test = create_rotated_dataset(transfer_test_augmented)

    # Define rotation prediction model
rotation_model = Sequential([
    Conv2D(32, 3, activation='silu', input_shape=(30, 30, 1), kernel_regularizer=l2_regularizer),
    Conv2D(32, 3, activation='silu', kernel_regularizer=l2_regularizer),
    MaxPooling2D(),
    Conv2D(32, 3, activation='silu', kernel_regularizer=l2_regularizer),
    Conv2D(32, 3, activation='silu', kernel_regularizer=l2_regularizer),
    MaxPooling2D(),
    Flatten(),
    Dense(256, activation='silu', kernel_regularizer=l2_regularizer),
    Dropout(0.33),
    Dense(4, activation='softmax')
])

rotation_model.compile(
    optimizer=RMSprop(learning_rate=0.0015),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

rotation_model.fit(
    rotated_train.batch(32),
    validation_data=rotated_test.batch(32),
    epochs=50,
    callbacks=[early_stopping, reduce_lr, model_checkpoint]
)
rotation_model.summary()
    # Fine-tuning: PCB Component Identification
feature_layers_2 = [
        Conv2D(32, 3, activation='silu', input_shape=(30, 30, 1), kernel_regularizer= l2_regularizer),
        Conv2D(32, 3, activation='silu', kernel_regularizer = l2_regularizer),
        MaxPooling2D(),
        Conv2D(32, 3, activation='silu', kernel_regularizer = l2_regularizer),
        Conv2D(32, 3, activation='silu', kernel_regularizer = l2_regularizer),
        MaxPooling2D(),
        Flatten(),
    ]

classification_layers_2 = [
        Dense(256, activation='silu', kernel_regularizer= l2_regularizer),
        Dense(256, activation='sigmoid', kernel_regularizer= l2_regularizer),
        Dropout(0.40),
        Dense(num_classes),
        Activation('softmax')
    ]

# Initialize main model with pretrained weights
main_model = Sequential(feature_layers_2 + classification_layers_2)

# Transfer weights from the rotation prediction model
for i, layer in enumerate(main_model.layers[:7]):  # Only transfer feature layers
    layer.set_weights(rotation_model.layers[i].get_weights())

main_model.compile(
    optimizer=RMSprop(learning_rate=0.001),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

main_model.fit(
        component_train_augmented,
        validation_data=component_test_augmented,
        epochs=50,
        callbacks=[early_stopping, reduce_lr, model_checkpoint]
)
main_model.summary()
# Fine-tune on transfer dataset
for layer in feature_layers_2:
    layer.trainable = False

transfer_model = Sequential(feature_layers_2 + classification_layers_2)

transfer_model.compile(
    optimizer=RMSprop(learning_rate=0.001),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

transfer_model.fit(
    transfer_train_augmented,
    validation_data= transfer_test_augmented,
    epochs=50,
    callbacks=[early_stopping, reduce_lr, model_checkpoint]
    )
transfer_model.summary()
