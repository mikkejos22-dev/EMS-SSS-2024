from __future__ import print_function
import tensorflow as tf
from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint
from keras.optimizers import SGD, RMSprop
from keras.src.layers import AveragePooling2D
from tensorflow.keras.datasets import cifar100
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt

main_directory = '30by30 images'  # Add personal directory here
transfer_directory = 'Transfer Dataset'  # Add personal directory here

component_train = tf.keras.preprocessing.image_dataset_from_directory(
    main_directory,
    validation_split=0.2,
    subset='training',
    seed=123,
    batch_size=16,
    color_mode='grayscale',
    image_size=(30, 30)
)

component_test = tf.keras.preprocessing.image_dataset_from_directory(
    main_directory,
    validation_split=0.3,
    subset='validation',
    seed=123,
    batch_size=16,
    color_mode='grayscale',
    image_size=(30, 30)
)

# Data Augmentation setup
datagen = ImageDataGenerator(
    rotation_range=25,
    width_shift_range=0.3,
    height_shift_range=0.3,
    shear_range=0.3,
    zoom_range=0.3,
    horizontal_flip=True,
    fill_mode='nearest',
    validation_split=0.3  # Reserve 30% of data for validation
)

# Modify dataset loading to include data augmentation
component_train_augmented = datagen.flow_from_directory(
    main_directory,
    target_size=(30, 30),
    color_mode='grayscale',
    batch_size=16,
    class_mode='sparse',
    subset='training',
    seed=123
)

component_test_augmented = datagen.flow_from_directory(
    main_directory,
    target_size=(30, 30),
    color_mode='grayscale',
    batch_size=16,
    class_mode='sparse',
    subset='validation',
    seed=123
)

transfer_train_augmented = datagen.flow_from_directory(
    transfer_directory,
    target_size=(30, 30),
    color_mode='grayscale',
    batch_size=16,
    class_mode='sparse',
    subset='training',
    seed=123
)

transfer_test_augmented = datagen.flow_from_directory(
    transfer_directory,
    target_size=(30, 30),
    color_mode='grayscale',
    batch_size=16,
    class_mode='sparse',
    subset='validation',
    seed=123
)

# Early stopping callback
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=3,  # Number of epochs with no improvement after which training will be stopped
    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity
)

model_checkpoint = ModelCheckpoint(
    'best_model.keras',
    monitor='val_loss',
    save_best_only=True
)

# Cache and prefetch setup
AUTOTUNE = tf.data.AUTOTUNE
component_train_augmented = component_train.cache().prefetch(buffer_size=AUTOTUNE)
component_test_augmented = component_test.cache().prefetch(buffer_size=AUTOTUNE)
transfer_train_augmented = component_train.cache().prefetch(buffer_size=AUTOTUNE)
transfer_test_augmented = component_test.cache().prefetch(buffer_size=AUTOTUNE)

# Reduce learning rate on plateau: helps with overfitting
reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.15,  # Factor by which the learning rate will be reduced
    patience=2,  # Number of epochs with no improvement after which learning rate will be reduced
    min_lr=0.000001  # Lower bound on the learning rate
)

num_classes = 3

feature_layers_2 = [
  Conv2D(32, 3, activation='silu', input_shape = (30,30,1)),
  Conv2D(32, 3, activation='silu'),
  MaxPooling2D(),
  Conv2D(32, 3, activation='silu'),
  Conv2D(32, 3, activation='silu'),
  MaxPooling2D(),
  Flatten(),
  ]

classification_layers_2 = [
  Dense(128, activation='silu'),
  Dense(128, activation='sigmoid'),
  Dropout(0.30),
  Dense(num_classes),
  Activation('softmax')
]

model_2 = Sequential(feature_layers_2 + classification_layers_2)

model_2.summary()

model_2.compile(
  optimizer='adam',
  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
  metrics=['accuracy'])

model_2.fit(
    component_train_augmented,
    validation_data=component_test_augmented,
    epochs=50,
    callbacks=[early_stopping, reduce_lr, model_checkpoint]
)

for layer in feature_layers_2:
    layer.trainable = False

model_2 = Sequential(feature_layers_2 + classification_layers_2)

model_2.summary()

model_2.compile(
  optimizer='adam',
  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
  metrics=['accuracy'])

model_2.fit(
  transfer_train_augmented,
  validation_data=transfer_test_augmented,
  epochs=3
)
