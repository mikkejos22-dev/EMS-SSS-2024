from __future__ import print_function
import tensorflow as tf
from keras.src.saving.saving_api import save_model
from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint
from keras.optimizers import RMSprop
from keras.src.layers import AveragePooling2D, GlobalAveragePooling2D, Reshape, Multiply, MaxPooling2D, MaxPooling1D, \
    GlobalAveragePooling1D, GlobalAveragePooling3D
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, Input
from tensorflow.keras.layers import Conv2D, AveragePooling2D
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.regularizers import l2
import tensorflow.keras.backend as K


main_directory = '30by30-images'  # Add personal directory here
transfer_directory = 'Transfer-Dataset'  # Add personal directory here

# Load data
component_train = tf.keras.preprocessing.image_dataset_from_directory(
    main_directory,
    validation_split=0.2,
    subset='training',
    seed=123,
    batch_size=16,
    color_mode='grayscale',
    image_size=(30, 30)
)

component_test = tf.keras.preprocessing.image_dataset_from_directory(
    main_directory,
    validation_split=0.3,
    subset='validation',
    seed=123,
    batch_size=16,
    color_mode='grayscale',
    image_size=(30, 30)
)
transfer_train = tf.keras.preprocessing.image_dataset_from_directory(transfer_directory,
                                                                      validation_split = 0.2,
                                                                      subset = 'training',
                                                                      seed = 123,
                                                                      batch_size = 16,
                                                                      color_mode = 'grayscale',
                                                                      image_size = (30,30)
                                                                     )
transfer_test = tf.keras.preprocessing.image_dataset_from_directory(transfer_directory,
                                                                      validation_split = 0.2,
                                                                      subset = 'validation',
                                                                      seed = 123,
                                                                      batch_size = 16,
                                                                      color_mode = 'grayscale',
                                                                      image_size = (30,30)
                                                                     )

# Data Augmentation setup
datagen = ImageDataGenerator(
    rotation_range=30,
    width_shift_range=0.25,
    height_shift_range=0.25,
    shear_range=0.25,
    zoom_range=0.25,
    horizontal_flip=True,
    fill_mode='nearest',
    validation_split=0.2
)

# Modify dataset loading to include data augmentation
component_train_augmented = datagen.flow_from_directory(
    main_directory,
    target_size=(30, 30),
    color_mode='grayscale',
    batch_size=16,
    class_mode='sparse',
    subset='training',
    seed=123
)

component_test_augmented = datagen.flow_from_directory(
    main_directory,
    target_size=(30, 30),
    color_mode='grayscale',
    batch_size=16,
    class_mode='sparse',
    subset='validation',
    seed=123
)

transfer_train_augmented = datagen.flow_from_directory(
    transfer_directory,
    target_size=(30, 30),
    color_mode='grayscale',
    batch_size=16,
    class_mode='sparse',
    subset='training',
    seed=123
)

transfer_test_augmented = datagen.flow_from_directory(
    transfer_directory,
    target_size=(30, 30),
    color_mode='grayscale',
    batch_size=16,
    class_mode='sparse',
    subset='validation',
    seed=123
)

# Early stopping callback
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=4,  # Number of epochs with no improvement after which training will be stopped
    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity
)

model_checkpoint = ModelCheckpoint(
    'best_model.keras',
    monitor='val_loss',
    save_best_only=True
)

# Cache and prefetch setup
AUTOTUNE = tf.data.AUTOTUNE
component_train_augmented = component_train.cache().prefetch(buffer_size=AUTOTUNE)
component_test_augmented = component_test.cache().prefetch(buffer_size=AUTOTUNE)
transfer_train_augmented = component_train.cache().prefetch(buffer_size=AUTOTUNE)
transfer_test_augmented = component_test.cache().prefetch(buffer_size=AUTOTUNE)

# Reduce learning rate on plateau: helps with overfitting
reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.17,  # Factor by which the learning rate will be reduced
    patience=2,  # Number of epochs with no improvement after which learning rate will be reduced
    min_lr=0.000001  # Lower bound on the learning rate
)

#l2_regularizer = l2(0.01)  # You can adjust the regularization factor

def Attention_Module(x):
    input_shape = K.int_shape(x)
    avg_Pool = GlobalAveragePooling2D()(x)
    dense_0 = Dense(512, activation='silu')(avg_Pool)
    dense_1 = Dense(256, activation= 'silu')(dense_0)
    dense_2 = Dense(input_shape[-1], activation='sigmoid')(dense_1)
    attention_scores = Reshape((1, 1, input_shape[-1]))(dense_2)  # Reshape to match channel dimension
    return Multiply()([x, attention_scores])

num_classes = 3
# Pretraining: Self-Supervised Rotation Prediction
def create_rotated_dataset(dataset):
    rotated_images = []
    rotated_labels = []
    rotations = [0, 90, 180, 270]
    for images, labels in dataset:
        for image in images:
            for i, angle in enumerate(rotations):
                rotated_images.append(tf.image.rot90(image, k=i))
                rotated_labels.append(i)
    return tf.data.Dataset.from_tensor_slices((rotated_images, rotated_labels))

rotated_train = create_rotated_dataset(transfer_train)
rotated_test = create_rotated_dataset(transfer_test)


input_layer = Input(shape=(30, 30, 1))
x = Conv2D(32, 3, activation='silu', input_shape=(30, 30, 1), )(input_layer)
x = Conv2D(32, 3, activation='silu')(x)
x = BatchNormalization()(x)
x = MaxPooling2D()(x)
x = Dropout(0.33)(x)
x = Conv2D(32, 3, activation='silu')(x)
x = Conv2D(32, 3, activation='silu')(x)
x = BatchNormalization()(x)
x = MaxPooling2D()(x)
x = Dropout(0.33)(x)
x = Conv2D(32, 3, activation='silu')(x)
x = BatchNormalization()(x)
x = MaxPooling2D()(x)
x = Attention_Module(x)
x = GlobalAveragePooling2D()(x)
output_layer = Dense(4, activation='softmax')(x)


rotation_model = Model(inputs=input_layer, outputs=output_layer)
rotation_model.compile(
    optimizer=RMSprop(learning_rate=0.001),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

rotation_model.fit(
    rotated_train.batch(32),
    validation_data=rotated_test.batch(32),
    epochs=50,
    callbacks=[early_stopping, reduce_lr]
)
rotation_model.summary()

    # Fine-tuning: PCB Component Identification
input_layer = Input(shape=(30, 30, 1))
x = Conv2D(32, 3, activation='silu', input_shape=(30, 30, 1), )(input_layer)
x = Conv2D(32, 3, activation='silu')(x)
x = BatchNormalization()(x)
x = MaxPooling2D()(x)
x = Dropout(0.33)(x)
x = Conv2D(32, 3, activation='silu')(x)
x = Conv2D(32, 3, activation='silu')(x)
x = BatchNormalization()(x)
x = MaxPooling2D()(x)
x = Dropout(0.33)(x)
x = Conv2D(32, 3, activation='silu')(x)
x = BatchNormalization()(x)
x = MaxPooling2D()(x)
x = Attention_Module(x)
x = GlobalAveragePooling2D()(x)
output_layer = Dense(3, activation='softmax')(x)

# Initialize main model with pretrained weights
main_model = Model(inputs=input_layer, outputs=output_layer)

# Transfer weights from the rotation prediction model
for i, layer in enumerate(main_model.layers[:9]):  # Only transfer feature layers
    layer.set_weights(rotation_model.layers[i].get_weights())
feature_layers= main_model.layers[:9]

main_model.compile(
    optimizer=RMSprop(learning_rate=0.0014),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

main_model.fit(
        component_train_augmented,
        validation_data=component_test_augmented,
        epochs=30,
        callbacks=[reduce_lr]
)
main_model.summary()
# Fine-tune on transfer dataset
for layer in feature_layers:
    layer.trainable = False

transfer_model = Model(inputs=input_layer, outputs=output_layer)

transfer_model.compile(
    optimizer=RMSprop(learning_rate=0.001),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

transfer_model.fit(
    transfer_train_augmented,
    validation_data= transfer_test_augmented,
    epochs=4,
    callbacks=[reduce_lr, model_checkpoint]
    )

transfer_model.summary()

save_model(transfer_model, 'Model-Files/transferModel.h5')
save_model(main_model, 'Model-Files/componentModel.h5')
